# AI Data Analysis Platform

## Overview

An AI-powered data analysis platform built with Streamlit that provides comprehensive analytics capabilities through Google's Gemini 2.5 Flash AI. The platform offers automated machine learning, interactive visualizations, text analytics, and intelligent data processing with a conversational AI interface. Users can upload datasets in multiple formats, perform automated ML model training and comparison, generate interactive dashboards, and export results in various formats including PDF reports, Excel files, and Jupyter notebooks.

## User Preferences

Preferred communication style: Simple, everyday language.

## System Architecture

### Frontend Architecture

**Framework**: Streamlit-based single-page application with responsive design
- **Rationale**: Streamlit provides rapid development for data science applications with built-in state management and reactive UI components
- **Layout**: Wide layout with expandable sidebar, responsive CSS for mobile/tablet/desktop breakpoints
- **State Management**: Session-based state management through Streamlit's `session_state` for maintaining user data, model results, and conversation history across interactions
- **UI Components**: Multi-column layouts, tabs for different analysis sections, and collapsible sections for clean information hierarchy

### Backend Architecture

**Core Processing Modules**:

1. **Data Processing Layer** (`modules/data_processing.py`)
   - Multi-format file parsing (CSV, Excel, JSON, Parquet, TSV)
   - Automated data profiling and quality assessment
   - Data cleaning with multiple strategies (imputation, outlier detection, deduplication)
   - Uses scikit-learn for preprocessing (StandardScaler, MinMaxScaler, LabelEncoder)

2. **Machine Learning Engine** (`modules/ml_models.py`)
   - Automated model training and comparison for classification and regression tasks
   - Supports 10+ algorithms: Random Forest, Logistic Regression, SVM, Ridge, Lasso, XGBoost, LightGBM
   - Optional hyperparameter tuning via GridSearchCV
   - Clustering (KMeans, DBSCAN) and dimensionality reduction (PCA)
   - Feature importance calculation and SHAP value integration
   - **Design Decision**: Uses ensemble of algorithms to provide users with best model recommendations rather than requiring manual selection

3. **Text Analytics Module** (`modules/text_analytics.py`)
   - Sentiment analysis using TextBlob for polarity and subjectivity scoring
   - Word cloud generation with customizable parameters
   - Topic extraction and text statistics
   - Categorical sentiment labeling (Positive/Negative/Neutral)

4. **Visualization Engine** (`modules/visualizations.py`)
   - Interactive plots using Plotly (scatter, bar, histogram, box, violin, heatmaps, line charts)
   - Specialized ML visualizations (confusion matrices, feature importance plots)
   - Responsive design with consistent theming
   - **Chosen over Matplotlib**: Plotly provides interactivity and better mobile responsiveness

5. **AI Chat Integration** (`modules/gemini_integration.py`)
   - Google Gemini 2.5 Flash integration with function calling capabilities
   - Conversational interface for data analysis queries
   - Context-aware responses using dataset metadata
   - Function calling for automated actions (profiling, cleaning, visualization requests)
   - **Design Pattern**: Tool-augmented generation where AI can trigger specific analysis functions

6. **Export Handler** (`modules/export_handler.py`)
   - Multi-format export: CSV, Excel, JSON, PDF reports
   - Model serialization using joblib
   - Jupyter notebook generation for reproducible analysis
   - FPDF-based PDF report generation

### Utility Layer

**Error Handling** (`utils/error_handler.py`):
- Centralized error logging to file system
- User-friendly error messages with actionable solutions
- Safe execution wrapper for all critical operations
- Data validation at entry points

**Rate Limiting** (`utils/rate_limiter.py`):
- Protects against Gemini API free tier limits (15 requests/minute, 1500/day)
- Session-based rate tracking using timestamps
- Queue system for managing pending requests
- Real-time feedback on remaining quota

**Helper Functions** (`utils/helpers.py`):
- Data type detection and column classification
- Quality score calculation algorithm
- Number formatting utilities
- Session state initialization

### Data Flow Architecture

1. **Upload**: User uploads file → Format detection → Pandas DataFrame creation
2. **Profile**: Automatic profiling triggers → Statistical analysis → Quality assessment
3. **Clean**: User selects cleaning strategies → Applied transformations → Updated dataset
4. **Analyze**: User requests analysis → ML models train OR visualizations generate OR AI chat processes query
5. **Export**: Results compilation → Format conversion → Download delivery

### Design Patterns

**Separation of Concerns**: Each module has single responsibility (data processing, ML, visualization, export)

**Defensive Programming**: Extensive validation at module boundaries with graceful degradation

**Progressive Enhancement**: Core functionality works without advanced features (XGBoost/LightGBM optional)

**Mobile-First Responsive Design**: CSS media queries adapt layout for different screen sizes

## External Dependencies

### AI & Machine Learning Services

**Google Gemini 2.5 Flash API**:
- Purpose: Conversational AI assistant with function calling
- Integration: `google-generativeai` Python client
- Authentication: API key via environment variable `GEMINI_API_KEY`
- Rate Limits: 15 requests/minute, 1500 requests/day (free tier)
- Use Case: Natural language queries about data, automated analysis suggestions

### Python Libraries

**Core Framework**:
- `streamlit`: Web application framework for the entire UI
- `pandas`: Primary data manipulation and analysis
- `numpy`: Numerical computations

**Machine Learning**:
- `scikit-learn`: ML algorithms, preprocessing, model evaluation
- `xgboost`: Gradient boosting (optional, graceful fallback)
- `lightgbm`: Fast gradient boosting (optional, graceful fallback)
- `joblib`: Model serialization

**Visualization**:
- `plotly`: Interactive plotting library
- `matplotlib`: Backend for some visualizations
- `wordcloud`: Text visualization

**Text Processing**:
- `textblob`: Sentiment analysis and NLP

**Export**:
- `fpdf`: PDF report generation
- `openpyxl`: Excel file operations

**Utilities**:
- Standard library modules: `os`, `json`, `datetime`, `logging`, `io`

### Deployment Configuration

**Railway Platform**:
- Build system: Nixpacks (configured via `railway.json`)
- Start command: `streamlit run app.py --server.port=$PORT --server.address=0.0.0.0 --server.headless=true`
- Restart policy: On failure with max 10 retries
- Environment variables required: `GEMINI_API_KEY`

### File System Dependencies

**Local Storage**:
- `app_errors.log`: Error logging file
- Temporary model files for serialization
- Session-based uploaded files (handled by Streamlit)

**Note**: No persistent database is used; all data processing is in-memory during session lifecycle.